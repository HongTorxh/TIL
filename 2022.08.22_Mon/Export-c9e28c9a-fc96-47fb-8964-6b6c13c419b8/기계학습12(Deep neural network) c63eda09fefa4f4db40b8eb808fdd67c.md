# 기계학습12(Deep neural network)

![스크린샷 2022-08-19 오후 6.31.45.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-19_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.31.45.png)

- History

![스크린샷 2022-08-19 오후 6.36.01.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-19_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.36.01.png)

- x1,x2 : feature가 2개인 것을 의미 ← 하나의 샘플에 대하여
- Perceptron (가장 기본 모델 )
    - 주어진 두개의 feature로부터 Y를 예측하는 모델을 하는데, 어떤 특정 모델을 가정하냐면, 각 feature마다 weight를 부여.
    - 가중치를 곱한 다음 각각을 합한 값이 threshold보다 작거나 같으면 0
    - 가중치를 곱한 다음 각각을 합한 값이 threshold보다 크면 1

![스크린샷 2022-08-19 오후 6.41.23.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-19_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.41.23.png)

- y절편(bias)을 둘때 더 정확한 모델을 만들 수 있다.

![스크린샷 2022-08-19 오후 6.42.14.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-19_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.42.14.png)

- 필요한 계산 : 각자 weight와 Feature를 곱해서 더하기

![스크린샷 2022-08-19 오후 6.46.06.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-19_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.46.06.png)

1. 알아내야 하는 parameter는 weight 값인 w들 ← 때문에 이것들을 0 근처에 매우 작은 random number들로 초기화
2. 각각 training sample x들에 대하여 예측되는 y값을 계산
3. wj를 업데이트
    1. learning rate : hyper parameter

![스크린샷 2022-08-19 오후 6.51.13.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-19_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.51.13.png)

1. w들을 랜덤하게 작은 값으로 초기화
2. 이 값을 기준으로 각각 샘플을 하나씩 넣어줌
3. $w^Tx$ 이후,
4. h()에 Input으로 넣어, 0인지 1인지 y^(hat)값을 계산
5. weight 값을 업데이트 y - y^(hat)
6. 수렴할때까지 반복

![스크린샷 2022-08-20 오후 4.25.57.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_4.25.57.png)

- 실제값과 예측값이 똑같을 시 $\delta w$가 0이 되고, 틀렸을시 $2 \eta x_j$가 된다.
- 이것이 더이상 업데이트 할 필요가 없어질때까지 반복
- Perceptron 모델의 경우 두 class가 충분히 linearly separable하고, learning rate가 충분히 작다면 수렴하는게 보장되어있다.

![스크린샷 2022-08-20 오후 4.39.55.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_4.39.55.png)

- 최적화 문제로 표현되는 모델의 파라미터를 구해낼때 사용될 수 있다.
- 최적화 하고 싶은 대상 값, 그 값을 목적함수(Objective function)이라고 부름.
- 보통 Error와 같은 그런 의미를 값을 칭함. (Cost function이라고도 불림)
- 중요한 개념 : 현재 파라미터로 계산한 outcome과 실제 label간에 차이의 제곱의 합으로 계산하는 것이 가장 일반적

![스크린샷 2022-08-20 오후 4.43.27.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_4.43.27.png)

![스크린샷 2022-08-20 오후 5.08.50.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.08.50.png)

- 최저점을 찾아가는게 목적
- 한번에 찾아갈 수 있을 때 : 2차식(미분해서 0이 되는 지점)
- Batch gradient descent : 모든 training sample을 고려하여 weight 값을 업데이트 하는 방식

![스크린샷 2022-08-20 오후 5.15.51.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.15.51.png)

- Epochs이 지날수록 error가 얼마나 줄어드는지 보여주는 그래프.
- 1번은 learning rate값이 너무 커서 error가 증가하고 있음.
- 2번은 learning rate값을 작게 하여 error값이 줄어들고는 있지만, 너무 천천히 수렴되고 있음.

![스크린샷 2022-08-20 오후 5.17.29.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.17.29.png)

- 출발지점에서 Gradient의 반대방향으로 천천히 감.
- Epochs를 적절한 값을 찾는것이 필요하다.

![스크린샷 2022-08-20 오후 5.18.43.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.18.43.png)

- Learning rate는 gradient 값 자체에도 영향이 있다. 둘을 곱하기 때문에.
- gradient가 결국엔 feature 값과 직접적으로 연관이 되게 되는데, feature값의 scale이 너무 달라지게 되면 적당한 learning rate를 구하는 것이 어려워지게 됨
- 결국엔 여기서도 feature를 normalization하는게 중요하다.

![스크린샷 2022-08-20 오후 5.22.14.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.22.14.png)

- Batch gradient descent 방식이 가장 기본적인 parameter를 얻어가는 방식
    - 전체 training set을 고려하여 weight 값을 업데이트하기 때문에, 굉장히 시간이 많이 걸릴 수 있다.
- 때문에, 이것을 효율적으로 하기 위해서 약간 변형시킨 Stochastic gradient descent를 실제로는 훨씬 많이 사용.
    - i번째 샘플을 한번 통과시킨 이후, gradient를 계산한 이후에 바로 weight를 업데이트
    - Batch만큼 수렴성이 잘 보장되지 않음
    - 비교적 잘 수렴하긴 하다.

![스크린샷 2022-08-20 오후 5.25.28.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.25.28.png)

- 업데이트가 더 자주 일어나다보니, SGD는 실제로 더 빨리 수렴하는 것을 볼 수 있다.

![스크린샷 2022-08-20 오후 5.26.50.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.26.50.png)

- 시간이 지날수록, epochs이 지날수록 learning rate값이 점점 작아져서 더 미세하게 잘 찾아갈 수 있도록 사용하고 있음.
- 적당한 c1,c2 값을 두고, iteration이 커질수록 learning rate가 작아지게 사용.

![스크린샷 2022-08-20 오후 5.29.23.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.29.23.png)

- SGD를 쓰다보면 너무 unstable해지는 단점이 있을 수 있다.
- Mini-batch : batch와 SGD의 중간쯤
- 작은 subset으로 나누어서 weight값을 업데이트
- 그냥 GD보다는 수렴이 빠르고, 좀 더 안정적

![스크린샷 2022-08-20 오후 5.32.53.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.32.53.png)

- Logistic regression : 분류를 위한 문제이지만, 특정 class에 속할 확률 값을 예측한다는 부분에서 regression이라는 이름으로 불림.

![스크린샷 2022-08-20 오후 5.35.48.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.35.48.png)

![스크린샷 2022-08-20 오후 5.40.20.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.40.20.png)

- Perceptron같은 기본 뉴런 모델을 통해 Boolean operation을 만들어볼 수 있음.
- bias를 잘 조정하면 AND, OR를 만들 수 있다.

![스크린샷 2022-08-20 오후 5.40.57.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.40.57.png)

![스크린샷 2022-08-20 오후 5.43.46.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.43.46.png)

![스크린샷 2022-08-20 오후 5.44.38.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.44.38.png)

- input features n개, sum하는 function, activation하는 Function
- activation하는 function은 linear 한것을 벗어나게, non-linearity한 관계를 심어주기 위해 사용 sun하는 function도 linear한데, activation하는 function도 linear하게 되버리면 일차함수를 합성해봤자 일차함수임.
- Output을 Input으로 하고 여러겹 쌓으면 network

![스크린샷 2022-08-20 오후 5.45.18.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.45.18.png)

- input layer → hidden layer → output layer
    - 인접하는 layer 노드들끼리 빽빽하게 있는 모델이 multi-level perceptron model, 고전적인 nueral network 모델
- non-linear function을 activation function으로 써서 복잡한 x,y 관계를 모델링
- 이러한 neural network을 training한다는 말은 link마다 붙어있는 weight parameter(input에)를 알아낸다는 것으로 알 수 있다.

![스크린샷 2022-08-20 오후 5.48.41.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.48.41.png)

![스크린샷 2022-08-20 오후 5.48.29.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.48.29.png)

- Network topology
    - hidden layer의 갯수 변경
    - 각 hidden layer안에서 node의 갯수 변경
    - 링크가 한뱡향인지, 재귀적인 부분이 있는지 information

![스크린샷 2022-08-20 오후 5.50.05.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.50.05.png)

- 일반적으로는 Feed-forword network ← 시그널이 한 뱡향으로만 흘러감
- But, Recurrent network도 있음 ← 시그널이 self-edge처럼 보임

![스크린샷 2022-08-20 오후 5.51.20.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.51.20.png)

![스크린샷 2022-08-20 오후 5.53.23.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.53.23.png)

![스크린샷 2022-08-20 오후 5.53.32.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.53.32.png)

![스크린샷 2022-08-20 오후 5.53.44.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.53.44.png)

![스크린샷 2022-08-20 오후 6.02.40.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.02.40.png)

- Neural network에서 필요한 실제 계산부분 : input이 주어져있을때 output 구하기

![스크린샷 2022-08-20 오후 6.07.06.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.07.06.png)

- Activation functions을 linear로 쓰게 되면, 모든 연산이 linear하기 때문에, 아무리 뉴런을 많이 놓고, 아무리 많은 layer를 해도 결국엔 linear한 function 하나만 나오게 됨.
- 가장 기본이 되는 Activation function : Unit step function(Perceptron에서 봤던)
    - 연속적이지 x
    - 0,1 discrete한 값을

![스크린샷 2022-08-20 오후 6.04.55.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.04.55.png)

- Sigmoid function : 고전적인 neural net에서 썼던 방법
    - 값을 0에서 1사이에 값으로 만들어줌
    - 연속적
    - 계속 미분 가능
    - 층을 깊게 쌓을 때 가장 문제가 되는 부분이, gradient 값이다.
        - 층을 깊게 쌓다보면은 chain rule에 의해 gradient 값을 계속 곱해야되는 연산이 있는데, 굉장히 0에 가까운 값을 계속 곱하다보면 gradient값이 0에 가까워짐, 그러면 weight parameter가 업데이트가 되지 않음.

![스크린샷 2022-08-20 오후 6.05.04.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.05.04.png)

![스크린샷 2022-08-20 오후 6.05.13.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.05.13.png)

- sigmoid와 비슷하지만, 기울기 값이 약간 다름.
- ReLU : 요즘 딥러닝 모델에서 가장 많이 사용하는 모델
    - 미분을 하면 0 또는 1로 값이 잘 나옴

![스크린샷 2022-08-20 오후 6.05.21.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.05.21.png)

- 마지막층에서 output 어떻게 도출?
    - regression : 수치적으로 구한 숫자 값을 그대로 output으로 둠
    - classification : softmax function

![스크린샷 2022-08-20 오후 6.05.30.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.05.30.png)

- Error에 해당하는 Function
    - MSE
    - Cross entropy

![스크린샷 2022-08-20 오후 6.05.41.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B812(Deep%20neural%20network)%20c63eda09fefa4f4db40b8eb808fdd67c/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2022-08-20_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.05.41.png)